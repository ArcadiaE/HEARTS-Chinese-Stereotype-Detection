{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d30152f-b9da-4d3b-92fe-09d66e89ee54",
   "metadata": {},
   "source": [
    "# AI for Sustainable Development – Coursework 2\n",
    "## HEARTS Text Stereotype Detection – Reproduction & Adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2e7603-ab8e-405b-9aaf-e358aa45cbbf",
   "metadata": {},
   "source": [
    "## Project Information\n",
    "- **Project:** HEARTS (Holistic Explainable and Robust Text Stereotype Detection)\n",
    "- **Dataset:** EMGSD (holistic-ai/EMGSD)\n",
    "- **Goal:** Reproduce original methodology + adapt to Chinese context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db27085d-1707-46ce-a6dd-42659160db1c",
   "metadata": {},
   "source": [
    "# Part A – Technical Implementation (70%)\n",
    "\n",
    "1. Reproduce baseline HEARTS AI methodology  \n",
    "2. Select a Chinese-context SDG problem  \n",
    "3. Build/curate a new dataset  \n",
    "4. Adapt model to new context  \n",
    "5. Evaluate & compare results  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae94b68b-00c0-4f29-86c7-add1224bb8d5",
   "metadata": {},
   "source": [
    "## 1. Reproducing the Baseline AI Methodology\n",
    "\n",
    "The HEARTS project trains stereotype-detection models on the EMGSD dataset using\n",
    "fine-tuned ALBERT-v2, DistilBERT, and BERT.  \n",
    "Key components:\n",
    "- EMGSD dataset (HuggingFace)\n",
    "- Fine-tuned ALBERT-v2 (baseline)\n",
    "- SHAP + LIME explainability pipeline\n",
    "- Token-level explanation similarity metrics:\n",
    "  - Cosine similarity\n",
    "  - Pearson correlation\n",
    "  - Jensen–Shannon divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59301ae9-2790-486d-bf51-c666d32b9bdc",
   "metadata": {},
   "source": [
    "### 1.1 Clone the original repository\n",
    "\n",
    "The original HEARTS repository is located at:\n",
    "\n",
    "https://github.com/holistic-ai/HEARTS-Text-Stereotype-Detection\n",
    "\n",
    "In your local environment:\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/holistic-ai/HEARTS-Text-Stereotype-Detection.git\n",
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1805c6-dd58-4768-a391-da9b71c23f09",
   "metadata": {},
   "source": [
    "### 1.2 Load the EMGSD Dataset from HuggingFace\n",
    "We directly load the dataset in Jupyter using the `datasets` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe39c5f5-326d-4c87-bf2f-6dd2791b3e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"holistic-ai/EMGSD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66a6c414-24ad-4bf0-aa88-40bdc1fac3a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stereotype_type': 'profession',\n",
       " 'text': 'The illustrator had tower hands.',\n",
       " 'text_with_marker': 'The illustrator had ===tower=== hands.',\n",
       " 'category': 'unrelated',\n",
       " 'data_source': 'stereoset_intrasentence',\n",
       " 'label': 'unrelated'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf77f2e-3cd6-4ff3-8e4f-b527c862b947",
   "metadata": {},
   "source": [
    "#### Data Structure Summary\n",
    "\n",
    "- `text`: input sentence\n",
    "- `label`: 1 = stereotype, 0 = neutral\n",
    "- Splits available: train / validation / test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb5e6f4-f3d0-4702-a159-33ce978f9e20",
   "metadata": {},
   "source": [
    "### 1.3 Data Preprocessing Overview\n",
    "\n",
    "Before training the ALBERT-v2 baseline model, the EMGSD dataset must be\n",
    "processed into a format suitable for the HEARTS framework.  \n",
    "The preprocessing stage includes:\n",
    "\n",
    "- Loading raw text and original labels  \n",
    "- Applying the ALBERT-v2 tokenizer to convert text into token IDs  \n",
    "- Padding/truncating each sample to a fixed length of 128 tokens  \n",
    "- Mapping the original string labels to binary labels:\n",
    "  - stereotype / related → 1  \n",
    "  - neutral / unrelated → 0  \n",
    "\n",
    "These steps prepare the dataset for fine-tuning the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b380183b-94f8-43ce-8ce1-e00e2e766faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 45760\n",
      "test size: 11441\n",
      "----------------------------------------\n",
      "train label distribution: Counter({'unrelated': 14992, 'neutral_nationality': 6942, 'stereotype_nationality': 6795, 'stereotype_profession': 5232, 'neutral_profession': 5186, 'stereotype_gender': 1709, 'neutral_gender': 1690, 'stereotype_lgbtq+': 885, 'neutral_lgbtq+': 842, 'stereotype_religion': 589, 'neutral_religion': 481, 'stereotype_race': 387, 'neutral_race': 30})\n",
      "test label distribution: Counter({'unrelated': 3781, 'stereotype_nationality': 1756, 'neutral_nationality': 1609, 'neutral_profession': 1284, 'stereotype_profession': 1238, 'stereotype_gender': 469, 'neutral_gender': 432, 'neutral_lgbtq+': 246, 'stereotype_lgbtq+': 203, 'neutral_religion': 170, 'stereotype_religion': 154, 'stereotype_race': 86, 'neutral_race': 13})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# 查看每个 split 的样本数\n",
    "for split in dataset.keys():\n",
    "    print(f\"{split} size:\", len(dataset[split]))\n",
    "\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# 查看每个 split 的标签分布\n",
    "for split in dataset.keys():\n",
    "    labels = [ex[\"label\"] for ex in dataset[split]]\n",
    "    print(f\"{split} label distribution:\", Counter(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48d5fd8-eeeb-4d62-9723-f764615c7f93",
   "metadata": {},
   "source": [
    "### 1.3.1 Tokenisation and Label Encoding (BERT-base-uncased)\n",
    "\n",
    "In this step, the EMGSD dataset is converted into the format required by the\n",
    "BERT-base-uncased baseline model.\n",
    "\n",
    "Steps:\n",
    "- Load the **BERT-base-uncased tokenizer** from HuggingFace.\n",
    "- Tokenise each sentence with truncation and padding (`max_length = 128`).\n",
    "- Add `input_ids` and `attention_mask` fields to the dataset.\n",
    "- Map the original string labels to binary numerical labels:\n",
    "  - stereotype / related → 1\n",
    "  - neutral / unrelated → 0\n",
    "\n",
    "The resulting `tokenized_dataset` will be used for fine-tuning the baseline\n",
    "classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb679fa7-a912-477f-b3f7-2cda5c663a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\PY\\Lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 45760\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 11441\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "# 如果你后面用 DistilBERT，就改成：\n",
    "# from transformers import DistilBertTokenizer\n",
    "# tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# 这里用 BERT 为例：\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def preprocess_batch(batch):\n",
    "    # 1) 文本 -> token\n",
    "    encodings = tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "    )\n",
    "\n",
    "    # 2) 字符串标签 -> 二分类 0/1\n",
    "    labels = []\n",
    "    for l in batch[\"label\"]:\n",
    "        if l is None:\n",
    "            # 极少数缺失值，直接当非刻板印象\n",
    "            labels.append(0)\n",
    "        elif isinstance(l, str):\n",
    "            # 所有 stereotype 开头的统统归为 1\n",
    "            if l.startswith(\"stereotype\"):\n",
    "                labels.append(1)\n",
    "            elif l == \"related\":\n",
    "                labels.append(1)\n",
    "            else:\n",
    "                # neutral / unrelated / 其他全部归为 0\n",
    "                labels.append(0)\n",
    "        else:\n",
    "            # 防御式写法，意外类型也归 0\n",
    "            labels.append(0)\n",
    "\n",
    "    encodings[\"labels\"] = labels\n",
    "    return encodings\n",
    "\n",
    "# 3) 对所有 split 应用预处理，生成 tokenized_dataset\n",
    "tokenized_dataset = dataset.map(preprocess_batch, batched=True)\n",
    "\n",
    "# 4) 只保留训练需要的字段，并设置为 torch 格式\n",
    "keep_cols = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "remove_cols = [c for c in tokenized_dataset[\"train\"].column_names if c not in keep_cols]\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(remove_cols)\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c7c269-2ec7-456e-826d-496abebf6e9f",
   "metadata": {},
   "source": [
    "### 1.4 Baseline Fine-tuning (BERT-base-uncased)\n",
    "\n",
    "Since the local environment does not reliably support SentencePiece, the\n",
    "baseline model is reproduced using **BERT-base-uncased**, which is also one of\n",
    "the transformer models evaluated in the HEARTS methodology.\n",
    "\n",
    "Training setup:\n",
    "- Model: BERT-base-uncased\n",
    "- Learning rate: 2e-5  \n",
    "- Batch size: 16  \n",
    "- Epochs: 3  \n",
    "- Weight decay: 0.01  \n",
    "- Max sequence length: 128  \n",
    "- Evaluation metrics: accuracy, precision, recall, F1  \n",
    "\n",
    "The HuggingFace `Trainer` API is used to fine-tune the model on the\n",
    "tokenised EMGSD dataset and to evaluate performance on the test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c77b8b-145b-4073-85ca-af582488600a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "E:\\PY\\Lib\\site-packages\\datasets\\load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "E:\\PY\\Lib\\site-packages\\datasets\\load.py:759: FutureWarning: The repository for precision contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/precision/precision.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "E:\\PY\\Lib\\site-packages\\datasets\\load.py:759: FutureWarning: The repository for recall contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/recall/recall.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "E:\\PY\\Lib\\site-packages\\datasets\\load.py:759: FutureWarning: The repository for f1 contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/f1/f1.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "E:\\PY\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='8580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  60/8580 09:11 < 22:29:23, 0.11 it/s, Epoch 0.02/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "# 1. 加载 BERT 分类模型\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "# 2. 定义评估指标\n",
    "accuracy_metric = load_metric(\"accuracy\")\n",
    "precision_metric = load_metric(\"precision\")\n",
    "recall_metric = load_metric(\"recall\")\n",
    "f1_metric = load_metric(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy_metric.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
    "        \"precision\": precision_metric.compute(predictions=preds, references=labels)[\"precision\"],\n",
    "        \"recall\": recall_metric.compute(predictions=preds, references=labels)[\"recall\"],\n",
    "        \"f1\": f1_metric.compute(predictions=preds, references=labels)[\"f1\"],\n",
    "    }\n",
    "\n",
    "# 3. 训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert-baseline\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\"   # 禁止 wandb / tensorboard / comet 等所有 loggers\n",
    ")\n",
    "\n",
    "# 4. 创建 Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"] if \"validation\" in tokenized_dataset else tokenized_dataset[\"train\"].select(range(500)),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 5. 训练\n",
    "trainer.train()\n",
    "\n",
    "# 6. 在测试集上评估\n",
    "test_results = trainer.evaluate(tokenized_dataset[\"test\"])\n",
    "test_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14a4459-7630-4536-b2a7-3bac19ca30db",
   "metadata": {},
   "source": [
    "### Baseline Results\n",
    "\n",
    "| Metric | Paper Result | Your Result |\n",
    "|--------|--------------|-------------|\n",
    "| Accuracy | | |\n",
    "| Precision | | |\n",
    "| Recall | | |\n",
    "| F1 | | |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9aaffa-a6d6-4e3f-8eee-7894b7ad3fb9",
   "metadata": {},
   "source": [
    "# 2. Contextualising the AI Method (China)\n",
    "\n",
    "We choose a Chinese stereotype detection problem aligned with **SDG 10: Reduced Inequalities**.\n",
    "\n",
    "Possible target groups:\n",
    "- Occupations (外卖员、工程师、教师)\n",
    "- Regional stereotypes（地域黑）\n",
    "- Gender stereotypes\n",
    "- Social groups\n",
    "\n",
    "Ethical considerations:\n",
    "- Remove personal identifiers  \n",
    "- Avoid sensitive political content  \n",
    "- Ensure dataset transparency  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54fc6f4-9f41-4d44-8cd3-cd7f11137cc6",
   "metadata": {},
   "source": [
    "# 3. New Dataset for Chinese Context\n",
    "\n",
    "### Data Sources\n",
    "- 微博公开评论  \n",
    "- Bilibili 评论  \n",
    "- 新闻标题  \n",
    "- 小红书短文片段  \n",
    "\n",
    "### Preprocessing\n",
    "- Remove usernames, links  \n",
    "- Chinese segmentation (jieba)  \n",
    "- Manual labeling of stereotype vs non-stereotype  \n",
    "- Train/val/test = 8/1/1  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5cd438-7afc-44d3-a859-cc144aecdce8",
   "metadata": {},
   "source": [
    "# 4. Model Adaptation (Chinese)\n",
    "\n",
    "We replace ALBERT-v2 with Chinese BERT models:\n",
    "- bert-base-chinese\n",
    "- hfl/chinese-roberta-wwm-ext\n",
    "\n",
    "Train using:\n",
    "- lr = 2e-5  \n",
    "- batch size = 16  \n",
    "- epochs = 3–5  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8ffb40-cca4-4934-91d7-692c6b2c56ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
