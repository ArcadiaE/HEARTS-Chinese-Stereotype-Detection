# Beyond English: Adapting HEARTS for Holistic Chinese Stereotype Detection

This repository contains the complete implementation for AISD Coursework 2, based on the HEARTS framework for explainable and sustainable stereotype detection. All technical work is performed inside **AISD_CW2_ver2.2.ipynb**, including English baseline replication, Chinese contextual adaptation, evaluation, and SHAP explainability.


## Project Overview

This project follows the coursework requirements:

- **Replicate the baseline methodology** using the EMGSD dataset with `bert-base-uncased`
- **Adapt the approach to a new local context** using the Chinese COLD dataset with `bert-base-chinese`
- **Implement Green AI methods** via partial unfreezing to reduce training cost
- **Provide explainability** using SHAP on both English and Chinese models
- **Evaluate and compare** the baseline and adapted models using accuracy, macro F1, and detailed inference examples

All steps—data loading, preprocessing, model training, tuning, evaluation, inference, and explainability—are implemented sequentially in the notebook.


## Repository Structure

```text
.
├── AISD_CW2_ver2.2.ipynb       # Main notebook (replication + adaptation + SHAP)
├── train.csv                   # COLD dataset (if provided)
├── dev.csv
├── test.csv
├── shap_analysis_cn_*.png      # Token contribution plots generated by the notebook
└── README.md
````

---

## Environment Setup

Install required dependencies:

```bash
pip install torch numpy pandas scikit-learn datasets transformers shap matplotlib
```

The notebook automatically detects GPU (`cuda`) or CPU.
SHAP for the Chinese model runs on CPU for stability.

---

## English Baseline Replication (EMGSD)

### Data

* Loaded via Hugging Face:

  ```python
  dataset = load_dataset("holistic-ai/EMGSD")
  ```
* Labels converted into binary stereotype vs non-stereotype categories
* Tokenised with `bert-base-uncased`, `max_length=128`

### Model

* Base: `BertForSequenceClassification(num_labels=2)`
* **Partial unfreezing strategy:**

  * Trainable: encoder layers **8–11**, pooler, classifier
  * Frozen: all lower layers
* Reduces trainable parameter ratio to align with sustainable AI practices

### Training

* Hugging Face `Trainer`
* 5 epochs, LR = 2e-5, weight decay = 0.1
* Mixed precision (fp16) when CUDA is available
* Early stopping
* Evaluated with accuracy and macro-F1
* Saved as:

  ```text
  ./final_balanced_model
  ```

### Explainability (English)

* SHAP text visualisation using:

  ```python
  shap.plots.text(shap_values)
  ```
* Highlights token contributions for stereotype classification

---

## Chinese Adaptation (COLD)

### Data Preparation

* Reads `train.csv`, `dev.csv`, `test.csv`
* Cleans text, removes empty lines and extremely short content
* Casts labels to integers
* Prints topic distribution and basic statistics
* Splits into stratified train–test sets
* Converts to Hugging Face `DatasetDict`
* If CSV files are missing, a synthetic placeholder dataset is generated to keep the notebook runnable (not used for final results)

### Model

* Tokeniser: `bert-base-chinese`
* Same partial unfreezing strategy as English baseline
* Prints total vs trainable parameter count

### Training

* Evaluation each epoch
* Early stopping (patience = 3)
* Standard accuracy + macro-F1 metrics
* Saved to:

  ```text
  ./final_chinese_holistic_model
  ```

### Inference

A helper function:

```python
predict_stereotype(text)
```

Classifies Chinese sentences across:

* Regional stereotypes
* Gender stereotypes
* Racial stereotypes
* Neutral cases

---

## Explainability for Chinese Model

Two SHAP methods are implemented:

### 1. Text-Level SHAP Visualisation

* Uses the saved pipeline to generate token-level explanations
* Displays highlight plots for stereotype-indicative words

### 2. Token Contribution Bar Plots

* Aggregates SHAP values for the stereotype class index
* Sorts token importance and produces bar charts
* Chinese fonts configured for correct rendering
* Saved as:

  ```text
  shap_analysis_cn_0.png
  shap_analysis_cn_1.png
  shap_analysis_cn_2.png
  ...
  ```

---

## How to Run

1. Clone repository
2. Install dependencies
3. Add COLD CSV files (optional but required for full results)
4. Run the notebook from top to bottom
5. Models and SHAP visualisations are generated during execution

---

## Notes

* The notebook is entirely self-contained
* All reproduction, adaptation, and explainability steps follow the coursework requirements
* No external scripts are required; all logic exists within a single notebook

---

```
```
